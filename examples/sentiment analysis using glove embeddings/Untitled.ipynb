{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "b'Skipping line 8836: expected 4 fields, saw 5\\n'\n",
      "b'Skipping line 535882: expected 4 fields, saw 7\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1193514 word vectors.\n",
      "WARNING:tensorflow:From /home/ayush99/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1255: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/ayush99/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1340: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Epoch 1/20\n",
      "45000/45000 [==============================] - 207s 5ms/step - loss: 0.6117 - acc: 0.6478\n",
      "Epoch 2/20\n",
      "45000/45000 [==============================] - 205s 5ms/step - loss: 0.5151 - acc: 0.7475\n",
      "Epoch 3/20\n",
      "45000/45000 [==============================] - 207s 5ms/step - loss: 0.4920 - acc: 0.7624\n",
      "Epoch 4/20\n",
      "45000/45000 [==============================] - 208s 5ms/step - loss: 0.4762 - acc: 0.7722\n",
      "Epoch 5/20\n",
      "45000/45000 [==============================] - 203s 5ms/step - loss: 0.4637 - acc: 0.7816\n",
      "Epoch 6/20\n",
      "45000/45000 [==============================] - 202s 4ms/step - loss: 0.4526 - acc: 0.7892\n",
      "Epoch 7/20\n",
      "45000/45000 [==============================] - 211s 5ms/step - loss: 0.4386 - acc: 0.7953\n",
      "Epoch 8/20\n",
      "45000/45000 [==============================] - 208s 5ms/step - loss: 0.4262 - acc: 0.8036\n",
      "Epoch 9/20\n",
      "45000/45000 [==============================] - 216s 5ms/step - loss: 0.4150 - acc: 0.8103\n",
      "Epoch 10/20\n",
      "45000/45000 [==============================] - 246s 5ms/step - loss: 0.4016 - acc: 0.8189\n",
      "Epoch 11/20\n",
      "45000/45000 [==============================] - 229s 5ms/step - loss: 0.3876 - acc: 0.8272\n",
      "Epoch 12/20\n",
      "45000/45000 [==============================] - 238s 5ms/step - loss: 0.3768 - acc: 0.8315\n",
      "Epoch 13/20\n",
      "45000/45000 [==============================] - 243s 5ms/step - loss: 0.3627 - acc: 0.8402\n",
      "Epoch 14/20\n",
      "45000/45000 [==============================] - 246s 5ms/step - loss: 0.3555 - acc: 0.8440\n",
      "Epoch 15/20\n",
      "45000/45000 [==============================] - 254s 6ms/step - loss: 0.3452 - acc: 0.8496\n",
      "Epoch 16/20\n",
      "45000/45000 [==============================] - 254s 6ms/step - loss: 0.3315 - acc: 0.8567\n",
      "Epoch 17/20\n",
      "45000/45000 [==============================] - 247s 5ms/step - loss: 0.3248 - acc: 0.8595\n",
      "Epoch 18/20\n",
      "45000/45000 [==============================] - 250s 6ms/step - loss: 0.3172 - acc: 0.8659\n",
      "Epoch 19/20\n",
      "45000/45000 [==============================] - 265s 6ms/step - loss: 0.3070 - acc: 0.8684\n",
      "Epoch 20/20\n",
      "45000/45000 [==============================] - 300s 7ms/step - loss: 0.2987 - acc: 0.8727\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import BatchNormalization, Flatten, Conv1D, MaxPooling1D\n",
    "from keras.layers import Dropout\n",
    "\n",
    "data = pd.read_csv(\"Sentiment Analysis Dataset.csv\", error_bad_lines=False)\n",
    "\n",
    "data_new = data[:50000]\t\n",
    "\n",
    "# define text data\n",
    "docs_combined = data_new['SentimentText']\n",
    "\n",
    "# initialize the tokenizer\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(docs_combined)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "\n",
    "# integer encode the text data\n",
    "encoded_docs = t.texts_to_sequences(docs_combined)\n",
    "\n",
    "# pad the vectors to create uniform length\n",
    "padded_docs_combined = pad_sequences(encoded_docs, maxlen=500, padding='post')\n",
    "\n",
    "# seperate the train and test sets\n",
    "\n",
    "df_train_padded = padded_docs_combined[:45000]\n",
    "df_test_padded = padded_docs_combined[45000:]\n",
    "\n",
    "df_train_y = []\n",
    "for ele in data_new[\"Sentiment\"][:45000]:\n",
    "    df_train_y.append([ele])\n",
    "\n",
    "# load the glove840B embedding into memory after downloading and unzippping\n",
    "\n",
    "embeddings_index = dict()\n",
    "f = open('glove/glove.twitter.27B.200d.txt')\n",
    "\n",
    "for line in f:\n",
    "    # Note: use split(' ') instead of split() if you get an error.\n",
    "\tvalues = line.split(' ')\n",
    "\tword = values[0]\n",
    "\tcoefs = np.asarray(values[1:], dtype='float32')\n",
    "\tembeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "\n",
    "# create a weight matrix\n",
    "embedding_matrix = np.zeros((vocab_size, 200))\n",
    "for word, i in t.word_index.items():\n",
    "\tembedding_vector = embeddings_index.get(word)\n",
    "\tif embedding_vector is not None:\n",
    "\t\tembedding_matrix[i] = embedding_vector\n",
    "\n",
    "model = Sequential()\n",
    "e = Embedding(vocab_size, 200, weights=[embedding_matrix],\n",
    "              input_length=500, trainable=False)\n",
    "model.add(e)\n",
    "model.add(Conv1D(128, 3, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(64, 3, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(64, 3, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "Adam_opt = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(optimizer=Adam_opt, loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "model.fit(df_train_padded, df_train_y, epochs=20, verbose=1)\n",
    "\n",
    "# save the data\n",
    "\n",
    "model.save_weights(\"weights.h5\")\n",
    "\n",
    "with open(\"structure.json\",\"w\") as f:\n",
    "    f.write(model.to_json())\n",
    "\n",
    "# Save test data\n",
    "\n",
    "df_test_y = []\n",
    "for ele in data_new[\"Sentiment\"][45000:]:\n",
    "\tdf_test_y.append([ele])\n",
    "\t\n",
    "np.save(\"Y.npy\", df_test_y)\n",
    "np.save(\"X.npy\", df_test_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
